import numpy as np
import pandas as pd
import json
import re
import torch
from typing import List, Dict, Any, Tuple
from tqdm import tqdm
import math

from chat_utils import get_chat_result
from config import config_mapping
from utils.tool_utils import Embedder


class TableRAGPipeline:
    """
    é›†æˆäº†ï¼šè¡¨æ ¼é‡æ„ã€BGE å‘é‡æ£€ç´¢ã€Schema Pruning (åˆ—ç­›é€‰) å’Œ å­è¡¨ç”Ÿæˆã€‚
    """
    def __init__(self,
                 df: pd.DataFrame,
                 external_text_list: List[str],  # æ ¸å¿ƒæ”¹åŠ¨ï¼šç›´æ¥è¾“å…¥å­—ç¬¦ä¸²åˆ—è¡¨
                 llm_backbone: str = "qwen2.5:32b",
                 llm_path: str = "./models/bge-m3"):  # æˆ–è€…ä½¿ç”¨æœ¬åœ°è·¯å¾„

        self.backbone = llm_backbone
        self.df = df
        self.raw_text_list = external_text_list
        # 1. åŠ è½½ LLM é…ç½®
        self.llm_config = config_mapping.get(llm_backbone)
        if not self.llm_config:
            raise ValueError(f"Backbone {llm_backbone} not found in config_mapping")

        # é¢„å¤„ç†ï¼šè½¬å­—ç¬¦ä¸²ï¼Œå¡«å……ç©ºå€¼
        self.df = self.df.astype(str).replace('nan', '')

        # 3. åŠ è½½ BGE Embedding æ¨¡å‹
        self.embedder = Embedder(llm_path)

        # 4. å†…éƒ¨çŠ¶æ€å­˜å‚¨
        self.documents = []  # å­˜å‚¨è½¬åŒ–åçš„å®ä½“æ–‡æ¡£
        self.table_embeddings = None  # è¡¨æ ¼è¡Œå‘é‡ (Tensor)
        self.text_embeddings = None  # æ–‡æœ¬å—å‘é‡
        self.template = ""  # å­˜å‚¨ç”Ÿæˆçš„é€šç”¨æ¨¡æ¿
        self.pk_col = self.df.columns[0] # é»˜è®¤ç¬¬ä¸€åˆ—ä¸ºä¸»é”®

    def _clean_json_response(self, content: str) -> Dict:
        """Helper: é²æ£’çš„ JSON æå–å™¨"""
        content = content.strip()
        match = re.search(r'(\{.*\})', content, re.DOTALL)
        json_str = match.group(1) if match else content
        try:
            return json.loads(json_str)
        except json.JSONDecodeError:
            print(f"âŒ JSON Parse Failed. Raw:\n{content}")
            return {}

    # =========================================================================
    # PHASE 1: ç¦»çº¿ç´¢å¼•æ„å»º (Offline Indexing)
    # =========================================================================

    def _generate_generic_template(self) -> Dict:
        """è®© LLM çœ‹è¡¨å¤´ï¼Œç”Ÿæˆä¸€ä¸ªé€šç”¨çš„ã€ä¸­ç«‹çš„è¡Œæè¿°æ¨¡æ¿"""
        columns = self.df.columns.tolist()
        prompt = """
You are a Data-to-Text Template Generator.
Input Columns: {columns}

Goal: Create a python f-string template to convert a table row into a natural language sentence.
Rules:
1. **Neutrality**: Do NOT infer or hallucinate. Just describe the data structure.
2. **Completeness**: You MUST include placeholders for ALL columns provided.
3. **Format**: Use {{Column Name}} for placeholders.
4. **Primary Key**: Identify the main entity column to start the sentence.

Output JSON only:
{{
  "primary_key": "<best identifier column>",
  "template": "<sentence template>"
}}
"""
        formatted_prompt = prompt.format(columns=', '.join(columns))
        print(f"ğŸ¤– [LLM] Generating generic row template...")
        response = get_chat_result(
            messages=[{"role": "user", "content": formatted_prompt}],
            tools=None,
            llm_config=self.llm_config
        )
        return self._clean_json_response(response.content)

    def build_index(self):
        """æ ¸å¿ƒæµç¨‹ï¼šæ‰§è¡Œç¦»çº¿å»ºåº“"""
        print("\n=== ğŸ—ï¸ Phase 1: Building Offline Index ===")

        # 1. ç”Ÿæˆæ¨¡æ¿
        template_info = self._generate_generic_template()
        self.template = template_info.get("template", "")
        self.pk_col = template_info.get("primary_key", self.df.columns[0])
        print(f"âœ… Template: {self.template}")

        # 2. è¡Œè½¬æ–‡æœ¬ (Entity Documents)
        py_template = self.template.replace("{{", "{").replace("}}", "}")
        self.documents = []
        for idx, row in tqdm(self.df.iterrows(), total=len(self.df), desc="Rows to Docs"):
            row_dict = row.to_dict()
            try:
                text = py_template.format(**row_dict)
                self.documents.append({
                    "row_id": idx,
                    "text": text,
                    "entity": row_dict.get(self.pk_col, "Unknown")
                })
            except Exception:
                continue

        # 3. BGE å‘é‡åŒ– (Vectorization)
        print("âš¡ Encoding with BGE...")
        table_texts = [d["text"] for d in self.documents]
        self.table_embeddings = torch.tensor(self.embedder.encode(table_texts))

        # å¯¹å¤–éƒ¨æ–‡æœ¬åˆ—è¡¨è¿›è¡Œå‘é‡åŒ–
        if self.raw_text_list and len(self.raw_text_list) > 0:
            print(f"âš¡ Encoding {len(self.raw_text_list)} External Text Blocks...")
            self.text_embeddings = torch.tensor(self.embedder.encode(self.raw_text_list))
        else:
            print("âš ï¸ Warning: external_text_list is empty, text indexing skipped.")

    # =========================================================================
    # PHASE 2: åœ¨çº¿æ¨ç† (Online Inference)
    # =========================================================================

    def _get_top_k_indices(self, query: str, embeddings: torch.Tensor, top_k: int) -> List[int]:
        """ç»Ÿä¸€æ£€ç´¢æ ¸å¿ƒï¼šå¤„ç† Query ç¼–ç ä¸ç›¸ä¼¼åº¦è®¡ç®—"""
        if embeddings is None: return []
        query_emb = torch.tensor(self.embedder.encode(query)).squeeze()
        # è®¡ç®—ç‚¹ç§¯ç›¸ä¼¼åº¦
        scores = torch.matmul(embeddings, query_emb)
        top_results = torch.topk(scores, k=min(top_k, embeddings.shape[0]))
        return top_results.indices.tolist()

    def _filter_columns(self, question: str) -> Dict[str, Any]:
        """è®© LLM æ ¹æ®é—®é¢˜ç­›é€‰åˆ—ï¼Œå¹¶åˆ¤æ–­æ˜¯å¦éœ€è¦è¡¨å¤–çŸ¥è¯†"""
        all_cols = self.df.columns.tolist()
        prompt = """
    You are a Column Selector.
    Question: "{question}"
    Available Columns: {columns}

    Goal: Select columns strictly necessary to answer the question.
    Rules:
    1. Include the Entity Name column.
    2. Include columns for filtering conditions in the question.
    3. CRITICAL: Include the column containing the Answer value.
    4. If the column that should contain the answer is missing, or the question asks for details not usually in a table (like "why", "how", or specific historical descriptions), set "answer_in_table" to true.

    Output JSON only:
    {{
      "selected_columns": ["<col1>", "<col2>", ...],
      "answer_in_table": true/false,
      "reasoning": "<brief explanation>"
    }}
    """
        formatted_prompt = prompt.format(question=question, columns=', '.join(all_cols))
        print(f"ğŸ¤– [LLM] Filtering columns & Checking self-sufficiency...")

        response = get_chat_result(
            messages=[{"role": "user", "content": formatted_prompt}],
            tools=None,
            llm_config=self.llm_config
        )

        result = self._clean_json_response(response.content)

        # æ ¡éªŒé€‰å‡ºçš„åˆ—æ˜¯å¦çœŸçš„åœ¨è¡¨ä¸­
        result["selected_columns"] = [c for c in result.get("selected_columns", []) if c in all_cols]
        if not result["selected_columns"]:
            result["selected_columns"] = all_cols

        print(f"ğŸ·ï¸ answer_in_table: {result['answer_in_table']}")
        print(f"ğŸ’¡ Reasoning: {result['reasoning']}")

        return result

    def _analyze_query_intent(self, question: str) -> Dict[str, bool]:
        """
        åˆ†æé—®é¢˜æ„å›¾ï¼šæ˜¯ç®€å•çš„æŸ¥å€¼ï¼Œè¿˜æ˜¯å¤æ‚çš„èšåˆ/æ’åº
        """
        signals = {
            "is_complex": False,
            "has_agg": any(w in question.lower() for w in ["how many", "sum", "average", "total", "percentage"]),
            "has_rank": any(w in question.lower() for w in ["most", "highest", "second", "rank", "top", "compare"])
        }
        if signals["has_agg"] or signals["has_rank"]:
            signals["is_complex"] = True
        return signals

    def _expand_context_radius(self, anchor_ids: List[int], intent: Dict[str, bool]) -> List[int]:
        """
        æ ¹æ®æ„å›¾å’Œåˆ†å¸ƒæƒ…å†µï¼Œè‡ªé€‚åº”åˆ†é…ä¸Šä¸‹æ–‡é¢„ç®—
        """
        final_ids = set(anchor_ids)

        # 1. è®¡ç®—åˆ†å¸ƒé›†ä¸­åº¦ (ç´¢å¼•çš„æ ‡å‡†å·®)
        std_dist = np.std(anchor_ids) if len(anchor_ids) > 1 else 0

        # 2. ç­–ç•¥ï¼šæŸ¥å€¼å‹ (é›†ä¸­ä¸”ç®€å•) -> å±€éƒ¨é‚»åŸŸæ‰©å±• (ä¸Šä¸‹å„1è¡Œ)
        if not intent["is_complex"] and std_dist < 5:
            print("ğŸ¯ Strategy: Compact Lookup (Expanding Local Neighborhood)")
            for rid in anchor_ids:
                if rid > 0: final_ids.add(rid - 1)
                if rid < len(self.df) - 1: final_ids.add(rid + 1)

        # 3. ç­–ç•¥ï¼šå¤æ‚å‹ (èšåˆ/æ’åº) -> å±æ€§å…±äº«æ‰©å±•
        else:
            print("ğŸ“Š Strategy: Analytical (Expanding by Key Attributes)")
            # æ‰¾åˆ°é”šç‚¹è¡Œä¸­æœ€é‡è¦çš„å±æ€§ï¼ˆæ¯”å¦‚åŒä¸€ä¸ª Engineï¼‰
            for rid in anchor_ids:
                # å‡è®¾æˆ‘ä»¬æ‹‰å…¥ä¸é”šç‚¹è¡Œå…±äº« 'Current layout engine' çš„æ‰€æœ‰è¡Œ
                # è¿™èƒ½å¸®åŠ© LLM åœ¨æ¯”è¾ƒæ—¶çœ‹åˆ°â€œåŒç±»â€æ•°æ®
                shared_val = self.df.iloc[rid].get('Current layout engine', '')
                if shared_val and shared_val != 'Unknown':
                    # æ‰¾åˆ°å…·æœ‰ç›¸åŒå¼•æ“çš„æ‰€æœ‰è¡Œç´¢å¼•
                    shared_rows = self.df[self.df['Current layout engine'] == shared_val].index.tolist()
                    final_ids.update(shared_rows)

        # é™åˆ¶æœ€å¤§ä¸Šä¸‹æ–‡é¢„ç®—ï¼Œé˜²æ­¢ Token æº¢å‡º (æ¯”å¦‚æœ€å¤š 15 è¡Œ)
        sorted_ids = sorted(list(final_ids))
        return sorted_ids[:15]

    # =========================================================================
    # PHASE 2: æ–‡æœ¬ä¾§ç²¾ç®€ (Textual Pruning - KV Focused)
    # =========================================================================

    def _retrieve_and_prune_text(self, question: str, anchor_entities: List[str], retrieved_texts: List[str]) -> str:
        """
        2. è‡ªåŠ¨åˆ¤å®š KV ç»“æ„ä¸å¥å­ç»“æ„
        3. åŸºäº BGE ç›¸ä¼¼åº¦ä¸å®ä½“é”šå®šæ‰“åˆ†
        4. åŠ¨æ€ä¿ç•™å‰ 50% çš„é«˜ä»·å€¼ä¿¡æ¯å•å…ƒ
        """

        all_units = []
        query_emb = self.embedder.encode(question, convert_to_tensor=True, normalize_embeddings=True)


        for text in retrieved_texts:
            # è‡ªåŠ¨åˆ¤å®š KV vs çº¯æ–‡æœ¬ç»“æ„
            is_kv = len(re.findall(r'[:ï¼š|]', text)) > len(text) / 50
            units = re.split(r'[\n;]', text) if is_kv else re.split(r'(?<=[ã€‚ï¼Ÿï¼?.])\s+', text)
            units = [u.strip() for u in units if len(u.strip()) > 5]

            if not units: continue

            # æ‰¹é‡è®¡ç®—å•å…ƒç›¸ä¼¼åº¦
            unit_embs = self.embedder.encode(units, convert_to_tensor=True, normalize_embeddings=True)
            scores = torch.matmul(unit_embs, query_emb).cpu().numpy()

            for i, score in enumerate(scores):
                text_unit = units[i]
                # å®ä½“é”šå®šåŠ åˆ†ï¼šå¦‚æœæåˆ°äº†è¡¨æ ¼é‡Œçš„ Top-K å®ä½“ï¼Œå¢åŠ æƒé‡
                entity_bonus = 0.2 if any(ent.lower() in text_unit.lower() for ent in anchor_entities) else 0.0
                all_units.append({"text": text_unit, "score": score + entity_bonus})

        # åŠ¨æ€æ¯”ä¾‹è£å‰ªï¼šä¿ç•™å‰ 50%
        all_units.sort(key=lambda x: x["score"], reverse=True)
        keep_count = math.ceil(len(all_units) * 0.5)
        top_units = all_units[:keep_count]

        print(f"âœ‚ï¸  Text Pruned: {len(all_units)} units -> Kept {len(top_units)} (Top 50%)")
        return "\n".join([f"- {u['text']}" for u in top_units])


    # =========================================================================
    # PHASE 3: æœ€ç»ˆèåˆæ¨ç† (Hybrid Inference)
    # =========================================================================
    def query(self, question: str, top_k_rows: int = 5) -> str:
        """
        æ¨ç†å…¥å£ï¼šç»“åˆè‡ªé€‚åº”å­è¡¨ä¸ç²¾ç®€ KV æ–‡æœ¬
        """
        print(f"\n=== ğŸš€ Hybrid Query: {question} ===")

        # 1. æ„å›¾åˆ†æä¸é”šç‚¹æ£€ç´¢
        intent = self._analyze_query_intent(question)
        anchor_ids = self._get_top_k_indices(question, self.table_embeddings, top_k=top_k_rows)
        anchor_entities = [self.df.iloc[rid][self.pk_col] for rid in anchor_ids]

        # 2. è‡ªé€‚åº”è¡ŒåŠå¾„æ‰©å±•
        expanded_ids = self._expand_context_radius(anchor_ids, intent)

        # 3. åŠ¨æ€åˆ—ç²¾ç®€
        col_info = self._filter_columns(question)
        print(f"ğŸ·ï¸  Ext Knowledge Required: {col_info.get('answer_in_table')}")

        # 4. æ„å»ºç²¾ç®€å­è¡¨
        sub_table_md = self.df.loc[expanded_ids, col_info["selected_columns"]].to_markdown(index=False)

        # 5. æ–‡æœ¬ä¾§æ£€ç´¢ä¸ 50% ç²¾ç®€
        pruned_text = ""
        if self.text_embeddings is not None:
            top_text_ids = self._get_top_k_indices(question, self.text_embeddings, top_k=3)
            retrieved_raw = [self.raw_text_list[i] for i in top_text_ids]
            print("self.raw_text_list",self.raw_text_list)
            print("retrieved_raw",retrieved_raw)

            pruned_text = self._retrieve_and_prune_text(question, anchor_entities, retrieved_raw)

        # 6. ç”Ÿæˆ
        final_prompt = f"""
    You are a factual reasoning assistant. Answer the question based on the two types of evidence provided below.

    ### 1. Structured Table Evidence (Key Rows & Columns)
    {sub_table_md}

    ### 2. Supporting Textual Evidence (Extracted Facts)
    {pruned_text}

    ### Task:
    - Combine the Table and Text to find the answer.
    - If the Table lacks a specific value (e.g., a version number), look for it in the Textual Evidence.
    - Question: {question}

    Answer:"""

        print("\nğŸ“ [Final Prompt Context Preview]:")
        print(f"--- Table ---\n{sub_table_md}\n--- Text ---\n{pruned_text}\n")

        # 4. ç”Ÿæˆç­”æ¡ˆ
        response = get_chat_result(
            messages=[{"role": "user", "content": final_prompt}],
            llm_config=self.llm_config
        )
        return response.content
