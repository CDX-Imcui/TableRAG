import numpy as np
import pandas as pd
import json
import re
import torch
import torch.nn.functional as F
from typing import List, Dict, Any, Tuple
from tqdm import tqdm
import math

from chat_utils import get_chat_result
from config import config_mapping
from utils.tool_utils import Embedder
from transformers import pipeline as hf_pipeline, AutoTokenizer, AutoModelForSequenceClassification
import time
from contextlib import contextmanager


@contextmanager
def timer(name):
    if torch.cuda.is_available():
        torch.cuda.synchronize()
    start = time.perf_counter()
    yield
    if torch.cuda.is_available():
        torch.cuda.synchronize()
    end = time.perf_counter()
    print(f"â±ï¸  [{name}] Time: {end - start:.4f}s")


class TableRAGPipeline:
    """
    é›†æˆäº†ï¼šè¡¨æ ¼é‡æ„ã€BGE å‘é‡æ£€ç´¢ã€Schema Pruning (åˆ—ç­›é€‰) å’Œ å­è¡¨ç”Ÿæˆã€‚
    """

    def __init__(self,
                 df: pd.DataFrame,
                 external_text_list: List[str],  # æ ¸å¿ƒæ”¹åŠ¨ï¼šç›´æ¥è¾“å…¥å­—ç¬¦ä¸²åˆ—è¡¨
                 llm_backbone: str = "qwen2.5:32b",
                 llm_path: str = "./models/bge-m3"):  # æˆ–è€…ä½¿ç”¨æœ¬åœ°è·¯å¾„

        self.backbone = llm_backbone
        self.df = df
        self.raw_text_list = external_text_list
        # 1. åŠ è½½ LLM é…ç½®
        self.llm_config = config_mapping.get(llm_backbone)
        if not self.llm_config:
            raise ValueError(f"Backbone {llm_backbone} not found in config_mapping")

        # é¢„å¤„ç†ï¼šè½¬å­—ç¬¦ä¸²ï¼Œå¡«å……ç©ºå€¼
        self.df = self.df.astype(str).replace('nan', '')

        # 3. åŠ è½½ BGE Embedding æ¨¡å‹
        self.embedder = Embedder(llm_path)

        # 4. å†…éƒ¨çŠ¶æ€å­˜å‚¨
        self.documents = []  # å­˜å‚¨è½¬åŒ–åçš„å®ä½“æ–‡æ¡£
        self.table_embeddings = None  # è¡¨æ ¼è¡Œå‘é‡ (Tensor)
        self.text_embeddings = None  # æ–‡æœ¬å—å‘é‡
        self.template = ""  # å­˜å‚¨ç”Ÿæˆçš„é€šç”¨æ¨¡æ¿
        self.pk_col = self.df.columns[0]  # é»˜è®¤ç¬¬ä¸€åˆ—ä¸ºä¸»é”®

        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.nli_model_name = "models/DeBERTa-v3-large-mnli-fever-anli-ling-wanli"
        self.nli_tokenizer = AutoTokenizer.from_pretrained(self.nli_model_name)
        self.nli_model = AutoModelForSequenceClassification.from_pretrained(self.nli_model_name).to(self.device)
        self.nli_model.eval()  # åŠ¡å¿…å¼€å¯ eval æ¨¡å¼ï¼Œå…³é—­ Dropout
        self.nli_labels = ["entailment", "neutral", "contradiction"]

    def _clean_json_response(self, content: str) -> Dict:
        """Helper: é²æ£’çš„ JSON æå–å™¨"""
        content = content.strip()
        match = re.search(r'(\{.*\})', content, re.DOTALL)
        json_str = match.group(1) if match else content
        try:
            return json.loads(json_str)
        except json.JSONDecodeError:
            print(f"âŒ JSON Parse Failed. Raw:\n{content}")
            return {}

    # =========================================================================
    # PHASE 1: ç¦»çº¿ç´¢å¼•æ„å»º (Offline Indexing)
    # =========================================================================

    def _generate_generic_template(self) -> Dict:
        """è®© LLM çœ‹è¡¨å¤´ï¼Œç”Ÿæˆä¸€ä¸ªé€šç”¨çš„ã€ä¸­ç«‹çš„è¡Œæè¿°æ¨¡æ¿"""
        columns = self.df.columns.tolist()
        prompt = """
You are a Data-to-Text Template Generator.
Input Columns: {columns}

Goal: Create a python format string to convert a table row into a natural language sentence.

CRITICAL RULES (Follow Strictness Level: MAX):
1. **DO NOT change column names.** Keep them EXACTLY as provided in the Input Columns.
2. **DO NOT replace spaces with underscores.**
   - WRONG: {{Software_license}}
   - CORRECT: {{Software license}}
3. Use double curly braces for placeholders: {{Column Name}}.
4. Do NOT infer or hallucinate information not present in the columns.

Output JSON only:
{{
  "primary_key": "<best identifier column>",
  "template": "<sentence template>"
}}
"""
        formatted_prompt = prompt.format(columns=', '.join(columns))
        print(f"ğŸ¤– [LLM] Generating generic row template...")
        response = get_chat_result(
            messages=[{"role": "user", "content": formatted_prompt}],
            tools=None,
            llm_config=self.llm_config
        )
        return self._clean_json_response(response.content)

    def build_index(self):
        """æ ¸å¿ƒæµç¨‹ï¼šæ‰§è¡Œç¦»çº¿å»ºåº“"""
        print("\n=== ğŸ—ï¸ Phase 1: Building Offline Index ===")

        # 1. ç”Ÿæˆæ¨¡æ¿
        template_info = self._generate_generic_template()
        self.template = template_info.get("template", "")
        self.pk_col = template_info.get("primary_key", self.df.columns[0])
        print(f"âœ… Template: {self.template}")

        # 2. è¡Œè½¬æ–‡æœ¬ (Entity Documents)
        py_template = self.template.replace("{{", "{").replace("}}", "}")
        self.documents = []
        table_texts = []
        for idx, row in tqdm(self.df.iterrows(), total=len(self.df), desc="Rows to Docs"):
            row_dict = row.to_dict()
            try:
                text = py_template.format(**row_dict)
                self.documents.append({
                    "row_id": idx,
                    "text": text,
                    "entity": row_dict.get(self.pk_col, "Unknown")
                })
                table_texts.append(text)
            except Exception:
                continue

        # 3. BGE å‘é‡åŒ– (Vectorization)
        print("âš¡ Encoding with BGE...")
        if not table_texts:
            raise ValueError("âŒ No texts generated from table! Check your template keys against dataframe columns.")
        raw_emb = torch.tensor(self.embedder.encode(table_texts))
        # æ‰‹åŠ¨è¿›è¡Œ L2 å½’ä¸€åŒ– (p=2, dim=1)
        self.table_embeddings = F.normalize(raw_emb, p=2, dim=1)

        # å¯¹å¤–éƒ¨æ–‡æœ¬åˆ—è¡¨è¿›è¡Œå‘é‡åŒ–
        if self.raw_text_list and len(self.raw_text_list) > 0:
            print(f"âš¡ Encoding {len(self.raw_text_list)} External Text Blocks...")
            self.text_embeddings = F.normalize(torch.tensor(self.embedder.encode(self.raw_text_list)), p=2, dim=1)
        else:
            print("âš ï¸ Warning: external_text_list is empty, text indexing skipped.")

    # =========================================================================
    # æ¨ç†
    # =========================================================================

    def _get_top_k_indices(self, query_emb: torch.Tensor, embeddings: torch.Tensor, top_k: int) -> List[int]:
        """ç»Ÿä¸€æ£€ç´¢æ ¸å¿ƒï¼šå¤„ç† Query ç¼–ç ä¸ç›¸ä¼¼åº¦è®¡ç®—"""
        if embeddings is None: return []
        # è®¡ç®—ç‚¹ç§¯ç›¸ä¼¼åº¦
        scores = torch.matmul(embeddings, query_emb)
        top_results = torch.topk(scores, k=min(top_k, embeddings.shape[0]))
        return top_results.indices.tolist()

    def _filter_columns(self, question: str) -> Dict[str, Any]:
        """è®© LLM æ ¹æ®é—®é¢˜ç­›é€‰åˆ—ï¼Œå¹¶åˆ¤æ–­æ˜¯å¦éœ€è¦è¡¨å¤–çŸ¥è¯†"""
        all_cols = self.df.columns.tolist()
        prompt = """
You are a Table Column Selector for table question answering.

Input:
- Question: "{question}"
- Available Columns: {columns}

Goal:
Select a MINIMALLY SUFFICIENT set of columns to answer the question using ONLY the table.
"Minimally sufficient" means the chosen columns are enough to:
(A) locate the target row(s),
(B) perform any required operations (filter/sort/rank/aggregate/compare),
(C) extract the final answer value.

Critical constraints:
1) You may ONLY choose from the provided column names and MUST preserve the exact column strings.
2) Always include at least one entity identifier / primary-key-like column (e.g., name/player/id) if such a column exists.
3) If the question involves ranking or "most/second/top", include BOTH:
   - the metric column (e.g., Yards/Score/Count), AND
   - the rank column, unless you are certain rank is derived from exactly that same metric.
4) IMPORTANT: If the final answer is NOT explicitly available in the table columns,
   OR the question requires external descriptive facts,
   set "answer_in_table" to false.
   If the table alone is sufficient, set "answer_in_table" to true.
5) Notes / remarks columns:
   Columns such as "Notes", "Remarks", "Comments", or similar
   should be kept by default if present

Output JSON only:
{{
  "selected_columns": ["<exact column name>", ...],
  "answer_in_table": true/false,
  "reasoning": "<brief explanation>"
}}
    """
        formatted_prompt = prompt.format(question=question, columns=', '.join(all_cols))
        response = get_chat_result(
            messages=[{"role": "user", "content": formatted_prompt}],
            tools=None,
            llm_config=self.llm_config
        )

        result = self._clean_json_response(response.content)

        # æ ¡éªŒé€‰å‡ºçš„åˆ—æ˜¯å¦çœŸçš„åœ¨è¡¨ä¸­
        result["selected_columns"] = [c for c in result.get("selected_columns", []) if c in all_cols]
        if not result["selected_columns"]:
            result["selected_columns"] = all_cols

        print(f"ğŸ·ï¸ answer_in_table: {result['answer_in_table']}")
        print(f"ğŸ’¡ Reasoning: {result['reasoning']}")

        return result

    def _analyze_query_intent(self, question: str) -> Dict[str, bool]:
        """
        åˆ†æé—®é¢˜æ„å›¾ï¼šæ˜¯ç®€å•çš„æŸ¥å€¼ï¼Œè¿˜æ˜¯å¤æ‚çš„èšåˆ/æ’åº
        """
        signals = {
            "is_complex": False,
            "has_agg": any(w in question.lower() for w in ["how many", "sum", "average", "total", "percentage"]),
            "has_rank": any(w in question.lower() for w in ["most", "highest", "second", "rank", "top", "compare"])
        }
        if signals["has_agg"] or signals["has_rank"]:
            signals["is_complex"] = True
        return signals

    def _expand_context_radius(self, anchor_ids: List[int], intent: Dict[str, bool]) -> List[int]:
        """
        æ ¹æ®æ„å›¾å’Œåˆ†å¸ƒæƒ…å†µï¼Œè‡ªé€‚åº”åˆ†é…ä¸Šä¸‹æ–‡é¢„ç®—
        """
        final_ids = set(anchor_ids)

        # åªè¦æ„å›¾æ˜¯ç®€å•çš„ï¼ˆæŸ¥å€¼ï¼‰ï¼Œå°±å¼ºåˆ¶èµ° Compact ç­–ç•¥ï¼Œå¿½ç•¥åˆ†å¸ƒç¦»æ•£åº¦ (std_dist)
        # åªæœ‰å½“é—®é¢˜ç¡®å®éœ€è¦èšåˆ/æ¯”è¾ƒ (is_complex=True) æ—¶ï¼Œæ‰è€ƒè™‘å±æ€§æ‰©å±•
        if not intent["is_complex"]:
            for rid in anchor_ids:
                if rid > 0: final_ids.add(rid - 1)
                if rid < len(self.df) - 1: final_ids.add(rid + 1)

        # å¤æ‚å‹ (èšåˆ/æ’åº) -> å±æ€§å…±äº«æ‰©å±•
        else:
            for rid in anchor_ids:
                shared_val = self.df.iloc[rid].get('Current layout engine', '')
                if shared_val and shared_val != 'Unknown':
                    shared_rows = self.df[self.df['Current layout engine'] == shared_val].index.tolist()
                    final_ids.update(shared_rows)

        sorted_ids = sorted(list(final_ids))
        return sorted_ids[:15]

    # =========================================================================
    # æ–‡æœ¬ä¾§ç²¾ç®€ (Textual Pruning - KV Focused)
    # =========================================================================

    def _retrieve_and_prune_text(self, query_emb: torch.Tensor, anchor_entities: List[str],
                                 retrieved_texts: List[str]) -> List[Dict]:
        """
        2. è‡ªåŠ¨åˆ¤å®š KV ç»“æ„ä¸å¥å­ç»“æ„
        3. åŸºäº BGE ç›¸ä¼¼åº¦ä¸å®ä½“é”šå®šæ‰“åˆ†
        4. åŠ¨æ€ä¿ç•™å‰ 50% çš„é«˜ä»·å€¼ä¿¡æ¯å•å…ƒ
        """
        if not retrieved_texts: return []

        entity_keywords = set()
        for ent in anchor_entities:
            for word in re.split(r'\W+', ent):  # æŒ‰éå­—æ¯å­—ç¬¦æ‹†åˆ†
                if len(word) > 3:  entity_keywords.add(word.lower())

        seen_units = set()  # ç”¨äºå»é‡
        for text in retrieved_texts:
            # è‡ªåŠ¨åˆ¤å®š KV vs çº¯æ–‡æœ¬ç»“æ„
            is_kv = len(re.findall(r'[:ï¼š|]', text)) > len(text) / 50
            units = re.split(r'[\n;]', text) if is_kv else re.split(r'(?<=[ã€‚ï¼Ÿï¼?.])\s+', text)
            for u in units:
                u_clean = u.strip()
                if len(u_clean) > 5 and u_clean not in seen_units:
                    seen_units.add(u_clean)

        unique_units = list(seen_units)
        if not seen_units: return []

        # å‘é‡åŒ– (å¢åŠ æ‰‹åŠ¨å½’ä¸€åŒ–ï¼Œç¡®ä¿åç»­è®¡ç®—å‡†ç¡®)
        # raw_embs: [N, Dim]
        raw_embs = torch.tensor(self.embedder.encode(unique_units))
        unit_embs = torch.nn.functional.normalize(raw_embs, p=2, dim=1)
        # æ‰“åˆ† (Query vs Units)
        if query_emb.dim() == 1:
            scores = torch.matmul(unit_embs, query_emb)
        else:
            scores = torch.matmul(unit_embs, query_emb.t()).squeeze()
        scores = scores.cpu().numpy()

        all_units = []
        for i, score in enumerate(scores):
            text_unit = unique_units[i]
            # å…³é”®è¯åŠ åˆ†
            if any(kw in text_unit.lower() for kw in entity_keywords):
                score += 0.2
            all_units.append({
                "text": text_unit,
                "score": score,
                "embedding": unit_embs[i]  # å¸¦å‡ºå‘é‡ï¼Œä¾›ä¸‹ä¸€æ­¥å¯¹é½ä½¿ç”¨
            })

        # ä¿ç•™å‰ 50%
        all_units.sort(key=lambda x: x["score"], reverse=True)
        keep_count = min(20, math.ceil(len(all_units) * 0.5))  # ç¨å¾®æ”¾å®½ä¸€ç‚¹ä¸Šé™åˆ°20ï¼Œä¿è¯ä¸Šä¸‹æ–‡

        return all_units[:keep_count]

    def _inject_cross_references(self, sub_df: pd.DataFrame, pruned_units: List[Dict]) -> Dict[str, str]:
        """
        æ ¸å¿ƒåŠŸèƒ½ï¼šå»ºç«‹åŒå‘å¼•ç”¨ (Bi-directional Reference)
        1. Table -> Text: åœ¨è¡¨æ ¼ä¸­æ·»åŠ æ–‡æœ¬ ID å’Œç›¸ä¼¼åº¦ (Top-5)ã€‚
        2. Text -> Table: åœ¨æ–‡æœ¬å‰æ ‡è®°å®ƒå±äºå“ªäº›å®ä½“ (Multi-label)ã€‚
        åˆ©ç”¨ Pruning é˜¶æ®µäº§ç”Ÿçš„å•å…ƒå‘é‡ï¼Œè®¡ç®—è¡¨æ ¼è¡Œä¸æ–‡æœ¬å•å…ƒçš„å¼•ç”¨å…³ç³»ã€‚
        """
        if not pruned_units:
            return {"table_md": sub_df.to_markdown(index=False), "text_str": ""}

        # 1. å‡†å¤‡æ•°æ®
        # æå–å•å…ƒå‘é‡å †å æˆçŸ©é˜µ [M, Dim]
        unit_embs = torch.stack([u['embedding'] for u in pruned_units])

        # æå–å­è¡¨è¡Œå‘é‡ [K, Dim]
        row_indices = sub_df.index.tolist()
        row_embs = self.table_embeddings[row_indices]  # æ³¨æ„ï¼štable_embeddings æœ€å¥½åœ¨ build_index é‡Œå·²ç»å½’ä¸€åŒ–

        # 2. è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ [K, M]
        sim_matrix = torch.matmul(row_embs, unit_embs.t())

        # 3. åŒå‘æ‰“æ ‡å®¹å™¨
        row_refs = {i: [] for i in range(len(sub_df))}  # è¡¨æ ¼è¡Œ -> å¼•ç”¨ID
        unit_labels = {j: set() for j in range(len(pruned_units))}  # æ–‡æœ¬å•å…ƒ -> å®ä½“å

        # 4. éå†è¡¨æ ¼è¡Œï¼Œå¯»æ‰¾åŒ¹é…çš„æ–‡æœ¬å•å…ƒ
        for r_idx in range(len(sub_df)):
            row_entity = str(sub_df.iloc[r_idx][self.pk_col])
            ent_keywords = {w.lower() for w in re.split(r'\W+', row_entity) if len(w) > 3}

            scores = sim_matrix[r_idx]

            # è¿™é‡Œçš„é˜ˆå€¼å¯ä»¥ç¨ä½ï¼Œå› ä¸º Pruning é˜¶æ®µå·²ç»ç­›é€‰è¿‡ä¸€è½®äº†
            # æ‰¾å‡º Top-5 ä¸”åˆ†æ•° > 0.45 çš„å•å…ƒ
            top_k_indices = torch.nonzero(scores > 0.45).squeeze()
            if top_k_indices.dim() == 0 and top_k_indices.item() is None: continue
            if top_k_indices.dim() == 0:
                top_k_indices = [top_k_indices.item()]
            else:
                top_k_indices = top_k_indices.tolist()

            # æŒ‰åˆ†æ•°æ’åºå– Top 5
            top_k_pairs = sorted([(scores[i].item(), i) for i in top_k_indices], key=lambda x: x[0], reverse=True)[:5]

            for score, u_idx in top_k_pairs:
                text_content = pruned_units[u_idx]['text']

                # åŒé‡æ ¡éªŒï¼šè¦ä¹ˆåˆ†æ•°æé«˜ï¼Œè¦ä¹ˆåŒ…å«å®ä½“å…³é”®è¯
                is_keyword_match = any(kw in text_content.lower() for kw in ent_keywords)
                is_high_conf = score > 0.75

                if is_keyword_match or is_high_conf:
                    # è¡¨æ ¼ä¾§è®°å½•: [0](0.82)
                    row_refs[r_idx].append(f"[{u_idx}]({score:.2f})")
                    # æ–‡æœ¬ä¾§è®°å½•: Android browser
                    unit_labels[u_idx].add(row_entity)

        # 5. ç”Ÿæˆå¢å¼ºç‰ˆè¡¨æ ¼
        view_df = sub_df.copy()
        view_df["Related Context IDs"] = [", ".join(refs) for refs in row_refs.values()]
        table_md = view_df.to_markdown(index=False)

        # 6. ç”Ÿæˆå¢å¼ºç‰ˆæ–‡æœ¬ä¸²
        formatted_texts = []
        for i, unit in enumerate(pruned_units):
            labels = sorted(list(unit_labels[i]))
            label_str = f"[Rel: {', '.join(labels)}]" if labels else ""
            # æ ¼å¼: [0] [Rel: Android] The text content...
            formatted_texts.append(f"[{i}] {label_str} {unit['text']}")

        return {
            "table_md": table_md,
            "text_str": "\n".join(formatted_texts)
        }

    def _verify_evidence(self, sub_table_facts: List[str], text_evidence: str) -> List[str]:
        """
        åˆ©ç”¨ Tokenizer çš„ Batch å¤„ç†èƒ½åŠ›ï¼Œä¸€æ¬¡æ€§æ ¡éªŒæ‰€æœ‰è¡¨æ ¼äº‹å®
        """
        if not text_evidence or not sub_table_facts:
            return []

        verification_signals = []
        # å°†æ–‡æœ¬è¯æ®ä½œä¸ºç»Ÿä¸€çš„å‰æ (Premise)
        premise = text_evidence[:1500]

        try:
            entail_idx = self.nli_labels.index("entailment")
            contra_idx = self.nli_labels.index("contradiction")
        except ValueError:
            # å…œåº•é€»è¾‘ï¼šå¦‚æœ labels è®¾ç½®ä¸å¯¹ï¼Œé»˜è®¤ä½¿ç”¨å®˜æ–¹æ ‡å‡† 0, 2
            entail_idx, contra_idx = 0, 2

        # 1. æ„é€  Batch è¾“å…¥å¯¹ï¼š[[Premise, Hypo1], [Premise, Hypo2], ...]
        pairs = [[premise, fact] for fact in sub_table_facts]

        # 2. è°ƒç”¨ Tokenizer çš„æ‰¹å¤„ç†åŠŸèƒ½
        # padding=True ä¼šè‡ªåŠ¨å¯¹é½é•¿åº¦ï¼Œreturn_tensors="pt" è¿”å› PyTorch å¼ é‡
        inputs = self.nli_tokenizer(
            pairs,
            padding=True,
            truncation=True,
            max_length=512,
            return_tensors="pt"
        ).to(self.device)

        # 3. å¼€å¯æ— æ¢¯åº¦æ¨ç†æ¨¡å¼
        with torch.no_grad():
            outputs = self.nli_model(**inputs)
            # å¯¹ logits åœ¨æœ€åä¸€ä¸ªç»´åº¦ï¼ˆæ ‡ç­¾ç»´åº¦ï¼‰åš Softmaxï¼Œå¾—åˆ°æ¦‚ç‡åˆ†å¸ƒ [Batch_size, 3]
            predictions = torch.softmax(outputs.logits, dim=-1)

        # 4. è§£æç»“æœ (å¯¹åº”å®˜æ–¹æ ‡ç­¾é¡ºåº: entailment, neutral, contradiction)
        # å°†ç»“æœè½¬å› CPU åˆ—è¡¨å¤„ç†
        predictions = predictions.cpu().numpy()

        for i, probs in enumerate(predictions):
            fact = sub_table_facts[i]
            entail_prob = probs[entail_idx]
            contra_prob = probs[contra_idx]

            # é˜ˆå€¼åˆ¤å®šï¼šåªæœ‰ç½®ä¿¡åº¦å¤Ÿé«˜æ‰è¾“å‡ºä¿¡å·ï¼Œå‡å°‘å™ªå£°
            if entail_prob > 0.7:
                verification_signals.append(f"âœ… Fact Verified: {fact[:60]}... (Conf: {entail_prob:.1%})")
            elif contra_prob > 0.7:
                verification_signals.append(f"âŒ Conflict Detected: {fact[:60]}... (Conf: {contra_prob:.1%})")

        return verification_signals

    # =========================================================================
    # æœ€ç»ˆèåˆæ¨ç† (Hybrid Inference)
    # =========================================================================
    def query(self, question: str) -> str:
        """
        æ¨ç†å…¥å£ï¼šç»“åˆè‡ªé€‚åº”å­è¡¨ä¸ç²¾ç®€ KV æ–‡æœ¬
        """
        print(f"\n=== ğŸš€ Hybrid Query: {question} ===")
        query_emb_numpy = self.embedder.encode(question)
        query_emb = torch.tensor(query_emb_numpy).squeeze()

        # 1. æ„å›¾åˆ†æä¸é”šç‚¹æ£€ç´¢
        intent = self._analyze_query_intent(question)
        anchor_ids = self._get_top_k_indices(query_emb, self.table_embeddings, top_k=10)
        anchor_entities = [self.df.iloc[rid][self.pk_col] for rid in anchor_ids]
        expanded_ids = self._expand_context_radius(anchor_ids, intent)

        # 3.  æ„å»ºç²¾ç®€å­è¡¨
        col_info = self._filter_columns(question)
        is_sufficient = col_info.get('answer_in_table', False)  # è·å–è¿™ä¸ªå…³é”®ä¿¡å·
        # å¦‚æœè¡¨é‡Œæ²¡ç­”æ¡ˆï¼Œå°±å¼ºåˆ¶å‘½ä»¤å®ƒå»æŒ–æ–‡æœ¬
        if not is_sufficient:
            guidance = "**CRITICAL**: The Table is KNOWN to lack the specific answer. You MUST extract the answer from the Textual Evidence."
        else:
            guidance = "**Note**: The Table likely contains the answer. Verify it against the Textual Evidence."

        # è·å–åŸºç¡€å­è¡¨æ•°æ®
        subtable_df = self.df.loc[expanded_ids, col_info["selected_columns"]]
        # æ–‡æœ¬æ£€ç´¢ä¸åŒå‘æ³¨å…¥
        final_table_md = ""
        pruned_text_str = ""

        pruned_text = ""
        if self.text_embeddings is not None:
            top_text_ids = self._get_top_k_indices(query_emb, self.text_embeddings, top_k=20)
            candidate_texts = [self.raw_text_list[i] for i in top_text_ids]
            # äº¤ç»™ pruning å‡½æ•°åšæœ€åçš„å†…å®¹ç²¾ç®€ (å– Top 50%)
            pruned_units = self._retrieve_and_prune_text(query_emb, anchor_entities, candidate_texts)

            # æ³¨å…¥å¼•ç”¨ä¿¡æ¯,åˆ©ç”¨ä¸Šä¸€æ­¥çš„å‘é‡åšè¡¨æ–‡å¯¹é½
            injection_result = self._inject_cross_references(subtable_df, pruned_units)
            final_table_md = injection_result["table_md"]
            pruned_text_str = injection_result["text_str"]
        else:
            final_table_md = subtable_df.to_markdown(index=False)

        # 6. NLI æ ¡éªŒä¸æ˜¾å¼æ‰“å°
        # relevant_docs = [d['text'] for d in self.documents if d['row_id'] in expanded_ids]
        # nli_signals = self._verify_evidence(relevant_docs, pruned_text)
        # if nli_signals:
        #     print(f"\nğŸ§  [NLI Logic Check] Found {len(nli_signals)} signals:")
        #     for s in nli_signals:
        #         print(f"  - {s}")
        # else:
        #     print("\nğŸ§  [NLI Logic Check] No strong entailment or contradiction found.")

        # 7. ç”Ÿæˆ
        final_prompt = f"""
    You are a factual reasoning assistant. Answer the question based on the evidence provided below.
    Rules:
1. **Check Table Sufficiency**: {guidance}

    ### 1. Structured Table Evidence (Key Rows & Columns)
    {final_table_md}
    ### 2. Supporting Textual Evidence (Extracted Facts)
    {pruned_text_str}
    - Question: {question}

PLEASE OUTPUT WITH THE FOLLOWING FORMAT:
<Answer>: [direct answer]
    """

        print("\nğŸ“ [Final Prompt Context Preview]:")
        print(f"--- Table ---\n{final_table_md}\n--- Text ---\n{pruned_text_str}\n")

        # 4. ç”Ÿæˆç­”æ¡ˆ
        response = get_chat_result(
            messages=[{"role": "user", "content": final_prompt}],
            llm_config=self.llm_config
        )

        return response.content
